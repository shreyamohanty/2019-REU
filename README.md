# 2019-REU
Collection of my code from my NSF Research Experience for Undergraduate Students at the University of North Texas

Machine learning already plays an integral part in several real world applications, from search engine recommendations to autonomous driving. However, a large chunk of data, such as medical records or transactional history, is sensitive and must be kept private. Such data can be protected by encryption, but performing machine learning over encrypted data poses an obstacle. Homomorphic encryption shows promising potential as a solution. Operations performed on homomorphically encrypted data will yield an encrypted result which can be decrypted by the original data owner. Thus, the original data need never be seen by a third party providing the machine learning services. Running machine learning algorithms on homomorphically encrypted data comes with some setbacks. Addition and multiplication are the only types of operations which can be performed on this data. Furthermore, encrypting data entails adding noise, which greatly increases the input’s size. Performing multiplication on this data augments noise and size even more, incurring a heavy computation tax. My research focused on tackling the latter problem. Specifically, I investigated ways to reduce plaintext data size while maintaining meaningful information and machine learning model accuracy. Reducing the size of an unencrypted dataset will in turn reduce size and computation tax for encrypted data. 

Tests were run on the IMDB review dataset provided by Keras, which contains 25,000 movie reviews labelled with either a positive or negative sentiment. 

I first examined trends arising from modifying the maximum length of a review to be read by the model, the maximum vocabulary size to use for the dataset, and the section of the review from which to read. The codes and descriptions for these tests are in keras_imdb, keras_imdb_reverse, keras_imdb_back, keras_imdb_middle, and keras_imdb_split.

Next, I examined the effect of removing stop words, a list of common and sometimes unimportant words generally removed from textual data during preprocessing. As stop words are some of the most common words in the English language, I first tested ignoring a certain number of top most frequent words. The code and descriptions for these tests are in keras_imdb_frequencies and keras_imdb_stopwords.

Next, I investigated using convolutional neural networks (CNNs) to shorten text data intelligently while maintaining meaningful information and found that using CNNs yielded a much higher model accuracy than without using them. The code and description for these tests are in keras_imdb_cnn.

Future work on this project involves creating a genetic algorithm to optimize a convolutional neural network to shorten text data. I read a paper (Sun, Yanan et. al. “Automatically Designing CNN Architectures Using Genetic Algorithm for Image Classification” https://arxiv.org/pdf/1808.03818.pdf) about using genetic algorithms to optimize a convolutional neural network architecture for image classification and began my own investigation into creating one for text classification. Although I haven’t completed the code, it is something I plan to continue working on. Currently I have created the population initialization function which creates a number of convolutional neural networks built out of layer blocks. The fitness function is simply a measure of model accuracy after being trained on the data. I am currently writing the crossover function, which essentially takes two parents, splits them at a random point, and combines opposite parents to create two children. The code and a more detailed description are in keras_imdb_ga. 
